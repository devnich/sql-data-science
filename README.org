#+STARTUP: fold indent
#+OPTIONS: tex:t toc:2 H:6 ^:{}

#+TITLE: Databases and SQL for Data Scientists
#+AUTHOR: Derek Devnich
#+BEGIN_SRC sql
#+END_SRC

* COMMENT SQL interaction in Emacs
1. Start SQLite inferior process
   ~M-x sql-sqlite~

2. Set SQL dialect for syntax highlighting
   ~M-x sql-set-product~
   ~sqlite~

* Introducing databases and SQL: Why use a database?
** Performance

** Correctness
There are two aspects of "correctness": Enforcing consistency and eliminating ambiguity. A database enforces consistency with a combination of data types, rules (e.g., foreign keys, triggers, etc.), and atomic transactions. It eliminates ambiguity by forbidding NULLs.

1. You can represent simple data in a single table
   [[file:images/animals.png]]

2. The single table breaks down when your data is complex
   [[file:images/animals_blob.png]]

   If you use a nested representation, the individual table cells are no longer atomic. The tool for query, search, or perform analyses rely on the atomic structure of the table, and they break down when the cell contents are complex.

3. Complex data with duplicate row
   [[file:images/animals_dup.png]]

   - Storing redundant information has storage costs
   - Redundant rows violate the Don't Repeat Yourself [DRY] principle. Every copy is an opportunity to introduce errors or inconsistencies into the data.
   - Storing multidimensional data in a single table increases the chance that your records will have NULL fields, which will complicate future queries (more on this later)

4. Solution: Normalize the data by breaking it into multiple tables

   | Animals                          | Sightings                          |
   |----------------------------------+------------------------------------+
   | [[file:images/animals_half.png]] | [[file:images/sightings_half.png]] |

   - Every row of every table contains unique information
   - Normalization is a continuum. We could normalize this data further, but there is a trade-off in terms of sane table management. Finding the correct trade-off is a matter of taste, judgment, and domain-specific knowledge.

** Encode Domain Knowledge
[[file:images/bank_account_schema.jpg]]

- Encodes shape of domain
- Embeds domain rules: e.g. cannot have a customer transaction without a customer account
- Rules provide additional layer of correctness in the form of constraints
- note that forbidding NULL seems much more reasonable in this context!

** Extensions
- Functions
- Data types (GIS, JSON, date/time, searchable document, currencyâ€¦)
- Full-text search

* Accessing data with queries
** Basic queries
1. Select everything from a table
   #+BEGIN_SRC sql
   SELECT *
   FROM surveys;
   #+END_SRC

2. Select a column
   #+BEGIN_SRC sql
   SELECT year
   FROM surveys;
   #+END_SRC

3. Select multiple columns
   #+BEGIN_SRC sql
   SELECT year, month, day
   FROM surveys;
   #+END_SRC

4. Limit results
   #+BEGIN_SRC sql
   SELECT *
   FROM surveys
   LIMIT 10;
   #+END_SRC

5. Get unique values
   #+BEGIN_SRC sql
   SELECT DISTINCT species_id
   FROM surveys;
   #+END_SRC

   #+BEGIN_SRC sql
   -- Return distinct pairs
   SELECT DISTINCT year, species_id
   FROM surveys;
   #+END_SRC

6. Calculate values
   #+BEGIN_SRC sql
   -- Convert kg to g
   SELECT plot_id, species_id, weight/1000
   FROM surveys;
   #+END_SRC

7. SQL databases have functions
   #+BEGIN_SRC sql
   SELECT plot_id, species_id, ROUND(weight/1000, 2)
   FROM surveys;
   #+END_SRC

** Filtering
1. Filter by a criterion
   #+BEGIN_SRC sql
   SELECT *
   FROM surveys
   WHERE species_id='DM';
   #+END_SRC

   #+BEGIN_SRC sql
   SELECT *
   FROM surveys
   WHERE year >= 2000;
   #+END_SRC

2. Combine criteria with booleans
   #+BEGIN_SRC sql
   SELECT *
   FROM surveys
   WHERE (year >= 2000) AND (species_id = 'DM');
   #+END_SRC

   #+BEGIN_SRC sql
   SELECT *
   FROM surveys
   WHERE (species_id = 'DM') OR (species_id = 'DO') OR (species_id = 'DS');
   #+END_SRC

** *Challenge 1*: Large bois
Get all of the individuals in Plot 1 that weighed more than 75 grams, telling us the date, species id code, and weight (in kg).

** Building complex queries
Use sets ("tuples") to condense criteria.
#+BEGIN_SRC sql
SELECT *
FROM surveys
WHERE (year >= 2000) AND (species_id IN ('DM', 'DO', 'DS'));
#+END_SRC

** Sorting
1. Sort by a column value
   #+BEGIN_SRC sql
   SELECT *
   FROM species
   ORDER BY taxa ASC;
   #+END_SRC

2. Descending sort
   #+BEGIN_SRC sql
   SELECT *
   FROM species
   ORDER BY taxa DESC;
   #+END_SRC

3. Nested sort
   #+BEGIN_SRC sql
   SELECT *
   FROM species
   ORDER BY genus ASC, species ASC;
   #+END_SRC

** *Challenge 2*
Write a query that returns year, species_id, and weight in kg from the surveys table, sorted with the largest weights at the top.

** Order of execution
Queries are pipelines
[[file:images/written_vs_execution_order.png]]

* Aggregating and grouping data (i.e. reporting)
** COUNT
#+BEGIN_SRC sql
SELECT COUNT(*)
FROM surveys;
#+END_SRC

#+BEGIN_SRC sql
-- SELECT only returns non-NULL results
SELECT COUNT(weight), AVG(weight)
FROM surveys;
#+END_SRC

** *Challenge 3*
1. Write a query that returns the total weight, average weight, minimum and maximum weights for all animals caught over the duration of the survey.
2. Modify it so that it outputs these values only for weights between 5 and 10.

** GROUP BY (i.e. summarize, pivot table)
1. Aggregate using GROUP BY
   #+BEGIN_SRC sql
   SELECT species_id, COUNT(*)
   FROM surveys
   GROUP BY species_id;
   #+END_SRC

2. Group by multiple nested fields
   #+BEGIN_SRC sql
   SELECT year, species_id, COUNT(*), AVG(weight)
   FROM surveys
   GROUP BY year, species_id;
   #+END_SRC

** Ordering aggregated results
#+BEGIN_SRC sql
SELECT species_id, COUNT(*)
FROM surveys
GROUP BY species_id
ORDER BY COUNT(species_id) DESC;
#+END_SRC

** Aliases
Create temporary variable names for future use. This will be useful later when we have to work with multiple tables.
1. Create alias for column name
   #+BEGIN_SRC sql
   SELECT MAX(year) AS last_surveyed_year
   FROM surveys;
   #+END_SRC

2. Create alias for table name
   #+BEGIN_SRC sql
   SELECT *
   FROM surveys AS surv;
   #+END_SRC

** The HAVING keyword
1. ~WHERE~ filters on database fields; ~HAVING~ filters on aggregations
   #+BEGIN_SRC sql
   SELECT species_id, COUNT(species_id)
   FROM surveys
   GROUP BY species_id
   HAVING COUNT(species_id) > 10;
   #+END_SRC

2. Using aliases to make results more readable
   #+BEGIN_SRC sql
   SELECT species_id, COUNT(species_id) AS occurrences
   FROM surveys
   GROUP BY species_id
   HAVING occurrences > 10;
   #+END_SRC

3. Note that in both queries, ~HAVING~ comes after ~GROUP BY~. One way to think about this is: the data are retrieved (~SELECT~), which can be filtered (~WHERE~), then joined in groups (~GROUP BY~); finally, we can filter again based on some of these groups (~HAVING~).

** *Challenge 4*
Write a query that returns, from the species table, the number of species in each taxa, only for the taxa with more than 10 species.

#+BEGIN_SRC sql
SELECT taxa, COUNT(*) AS n
FROM species
GROUP BY taxa
HAVING n > 10;
#+END_SRC

** Saving queries for future use
A view is a permanent query; alternatively, it is a table that auto-refreshes based on the contents of other tables.
1. A sample query
   #+BEGIN_SRC sql
   SELECT *
   FROM surveys
   WHERE year = 2000 AND (month > 4 AND month < 10);
   #+END_SRC

2. Save the query permanently as a view
   #+BEGIN_SRC sql
   CREATE VIEW summer_2000 AS
   SELECT *
   FROM surveys
   WHERE year = 2000 AND (month > 4 AND month < 10);
   #+END_SRC

3. Query the view (i.e. the query results) directly
   #+BEGIN_SRC sql
   SELECT *
   FROM summer_2000
   WHERE species_id = 'PE';
   #+END_SRC

** NULL
Start with slides: NULLs are missing data and give deceptive query results. Then demo:
1. Count all the things
   #+BEGIN_SRC sql
   SELECT COUNT(*)
   FROM summer_2000;
   #+END_SRC

2. Count all the not-females
   #+BEGIN_SRC sql
   SELECT COUNT(*)
   FROM summer_2000
   WHERE sex != 'F';
   #+END_SRC

3. Count all the not-males. These two do not add up!
   #+BEGIN_SRC sql
   SELECT COUNT(*)
   FROM summer_2000
   WHERE sex != 'M';
   #+END_SRC

4. Explicitly test for NULL
   #+BEGIN_SRC sql
   SELECT COUNT(*)
   FROM summer_2000
   WHERE sex != 'M' OR sex IS NULL;
   #+END_SRC

* Combining data with joins
** (Inner) joins
1. Join on fully-identified table fields
   #+BEGIN_SRC sql
   SELECT *
   FROM surveys
   JOIN species
   ON surveys.species_id = species.species_id;
   #+END_SRC

2. Join a subset of the available columns
   #+BEGIN_SRC sql
   SELECT surveys.year, surveys.month, surveys.day, species.genus, species.species
   FROM surveys
   JOIN species
   ON surveys.species_id = species.species_id;
   #+END_SRC

3. Join on table fields with identical names
   #+BEGIN_SRC sql
   SELECT *
   FROM surveys
   JOIN species
   USING (species_id);
   #+END_SRC

** *Challenge 5*
Write a query that returns the genus, the species name, and the weight of every individual captured at the site.

#+BEGIN_SRC sql
SELECT species.genus, species.species, surveys.weight
FROM surveys
JOIN species
ON surveys.species_id = species.species_id;
#+END_SRC

** Other join types
Slides: Talk about the structure of joins

** Combining joins with sorting and aggregation
#+BEGIN_SRC sql
SELECT plots.plot_type, AVG(surveys.weight)
FROM surveys
JOIN plots
ON surveys.plot_id = plots.plot_id
GROUP BY plots.plot_type;
#+END_SRC

** *(Optional) Challenge 6*
Write a query that returns the number of animals caught of each genus in each plot. Order the results by plot number (ascending) and by descending number of individuals in each plot.

#+BEGIN_SRC sql
SELECT surveys.plot_id, species.genus, COUNT(*) AS number_indiv
FROM surveys
JOIN species
ON surveys.species_id = species.species_id
GROUP BY species.genus, surveys.plot_id
ORDER BY surveys.plot_id ASC, number_indiv DESC;
#+END_SRC

** (Optional) Functions COALESCE and NULLIF
1. Replace missing values (NULL) with a preset value using COALESCE
   #+BEGIN_SRC sql
   SELECT species_id, sex, COALESCE(sex, 'U')
   FROM surveys;
   #+END_SRC

2. Replacing missing values allows you to include previously-missing rows in joins
   #+BEGIN_SRC sql
   SELECT surveys.year, surveys.month, surveys.day, species.genus, species.species
   FROM surveys
   JOIN species
   ON COALESCE(surveys.species_id, 'AB') = species.species_id;
   #+END_SRC

3. NULLIF is the inverse of COALESCE; you can mask out values by converting them to NULL
   #+BEGIN_SRC sql
   SELECT species_id, plot_id, NULLIF(plot_id, 7)
   FROM surveys;
   #+END_SRC

* COMMENT (Optional) Creating and modifying data
** Create tables
** Insert statements
** Update
** Referential Integrity
Data integrity constraints: Keys, not null, data types, etc
** Table contraints
sqlite check command
https://stackoverflow.com/questions/29476818/how-to-avoid-inserting-the-wrong-data-type-in-sqlite-tables
https://www.sqlitetutorial.net/sqlite-check-constraint/
** Atomic commits
By default, each INSERT statement is its own transaction. But if you surround multiple INSERT statements with BEGIN...COMMIT then all the inserts are grouped into a single transaction. The time needed to commit the transaction is amortized over all the enclosed insert statements and so the time per insert statement is greatly reduced.

* (Optional) SQLite on the command line
** Basic commands
#+BEGIN_SRC bash
sqlite3     # enter sqlite prompt
.tables     # show table names
.schema     # show table schema
.help       # view built-in commands
.quit
#+END_SRC

** Getting output
1. Formatted output in the terminal
   #+BEGIN_SRC sql
   .headers on
   .help mode
   .mode column
   #+END_SRC

   #+BEGIN_SRC sql
   select * from species where taxa == 'Rodent';
   #+END_SRC

2. Output to .csv file
   #+BEGIN_SRC bash
   .mode csv
   .output test.csv
   #+END_SRC

   #+BEGIN_SRC sql
   select * from species where taxa == 'Rodent';
   #+END_SRC

   #+BEGIN_SRC bash
   .output stdout
   #+END_SRC

* (Optional) Database access via programming languages
** R language bindings
1. Resources
   - https://www.r-project.org/nosvn/pandoc/RSQLite.html
   - https://rsqlite.r-dbi.org/reference/sqlite
   - https://dbi.r-dbi.org

2. Import libraries
   #+BEGIN_SRC R
   library("DBI")
   library("RSQLite")
   #+END_SRC

3. FYI, use namespaces explicitly
   #+BEGIN_SRC R
   con <- DBI::dbConnect(RSQLite::SQLite(), "../data/portal_mammals.sqlite")
   #+END_SRC

4. Show tables
   #+BEGIN_SRC R
   dbListTables(con)
   #+END_SRC

5. Show column names
   #+BEGIN_SRC R
   dbListFields(con, "species")
   #+END_SRC

6. Get query results at once
   #+BEGIN_SRC R
   df <- dbGetQuery(con, "SELECT * FROM surveys WHERE year = 2000")
   head(df)
   #+END_SRC

7. Use parameterized queries
   #+BEGIN_SRC R
   df <- dbGetQuery(con, "SELECT * FROM surveys WHERE year = ? AND (month > ? AND month < ?)",
                    params = c(2000, 4, 10))
   head(df)
   #+END_SRC

8. Disconnect
   #+BEGIN_SRC R
   dbDisconnect(con)
   #+END_SRC

** COMMENT Python language bindings

* (Optional) What kind of data storage system do I need?
** Non-atomic write; sequential read
1. Files

** Single atomic write (database-level lock); query-driven read
1. SQLite
2. Microsoft Access

** Multiple atomic writes (row-level lock); query-driven read
1. PostgreSQL: https://www.postgresql.org
2. MySQL/MariaDB
   - https://mariadb.org
   - https://www.mysql.com
3. Oracle
4. Microsoft SQL Server
5. ...etc.

* (Optional) Performance tweaks and limitations
** Getting the most out of your database
1. Use recommended settings, not default settings
2. Make judicious use of indexes
3. Use the query planner (this will provide feedback for item 2)
4. Cautiously de-normalize your schema

** Where relational databases break down
1. Very large data (hardware, bandwidth, and data integration problems)
2. Distributed data (uncertainty about correctness)

** Why are distributed systems hard?
1. CAP theorem
   - In theory, pick any two: Consistent, Available, Partition-Tolerant
   - In practice, Consistent or Available in the presence of a Partition

2. Levels of data consistency
   - https://jepsen.io/consistency
   - https://github.com/aphyr/distsys-class

3. Fallacies of distributed computing
   1. The network is reliable
   2. Latency is zero
   3. Bandwidth is infinite
   4. The network is secure
   5. Topology doesn't change
   6. There is one administrator
   7. Transport cost is zero
   8. The network is homogeneous

* *Endnotes*
* Credits
- Data management with SQL for ecologists: https://datacarpentry.org/sql-ecology-lesson/
- Databases and SQL: http://swcarpentry.github.io/sql-novice-survey/ (data hygiene, creating and modifying data)
- Simplified bank account schema: https://soft-builder.com/bank-management-system-database-model/
- Botanical Information and Ecology Network schema: https://bien.nceas.ucsb.edu/bien/biendata/bien-3/bien-3-schema/

* References
- C. J. Date, /SQL and Relational Theory/: https://learning.oreilly.com/library/view/sql-and-relational/9781491941164/
- Common database mistakes: https://stackoverflow.com/a/621891
- Fallacies of distributed computing: https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing

* Data Sources
- Portal Project Teaching Database: https://figshare.com/articles/dataset/Portal_Project_Teaching_Database/1314459
  Specifically, portal_mammals.sqlite: https://figshare.com/ndownloader/files/11188550

* COMMENT Export to Markdown using Pandoc
  Do this if you want code syntax highlighting and a table of contents on Github.
** Generate generic Markdown file
#+BEGIN_SRC bash
# pandoc README.org -o tmp.md --wrap=preserve
pandoc README.org -o tmp.md
#+END_SRC

** Edit generic Markdown file to remove illegal front matter
1. Org directives
2. Anything that isn't part of the document structure (e.g. TODO items)

** Generate Github Markdown with table of contents
#+BEGIN_SRC bash
# pandoc -f markdown --toc --toc-depth=2 --wrap=preserve -s tmp.md -o README.md
pandoc -f markdown --toc --toc-depth=2 -s tmp.md -o README.md
#+END_SRC

** Find and replace code block markers in final document (if applicable)
#+BEGIN_EXAMPLE
M-x qrr " {.python}" "python"
M-x qrr " {.bash}" "bash"
#+END_EXAMPLE

** Fix any tables
